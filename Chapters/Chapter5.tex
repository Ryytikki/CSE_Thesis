\chapter{Conclusion and Future Development}

\section{Conclusion}

In this thesis we discussed radiative transfer in astrophysics, listing the common algorithmic methods for application in astrophysical simulations. We presented an overview of reverse ray tracing, its benefits and limitations compared to forward ray tracing as well as the existing codes that employ it before giving a summary of the TREVR radiative transfer method. We gave details of the GASOLINE astrophysical simulation code as well as the use of trees for source merging. Reverse ray tracing is an area of active interest in the astrophysical simulation community but techniques are still being developed and code papers to date tend to be incomplete, with few tests that fully study their accuracy and scaling. Much of reverse ray tracing was developed for post processing, where crude accuracy was acceptable or one could change parameters until desired results were achieved. Now that it is being simulated during run time, often with the output directly linked to other systems such as cooling, this lack of accuracy has become an issue. In particular, we identified an issue with complex sources that no previous code has addressed. This was detailed in Chapter \ref{sec:complexsources}.

We presented the ChaNGa radiative transfer code and the Charm++ framework that it utilized, comparing it directly to GASOLINE, particularly in areas such as the combined tree build and top down tree walk. We described the core components of the Charm++ framework that allowed for us to simplify the development process, such as Chare and Message objects. As part of this development we realized that for all the appeal of the Charm++ framework, the final products, e.g. ChaNGa, are unwieldy and more challenging to develop compared to other codes such as GASOLINE. We strongly advocate that ChaNGa needs to be refactored to improve usability, saving substantial work for future developers. We note that this has already been done for SPH in ChaNGa, resulting in a clean implementation that is likely better than the GASOLINE implementation to work on.

We discussed the development process for both the optically thin and optically thick walks, the methods for calculating the flux and optical depth of the ray, as well as their refinement criterion $\theta_{open}$ and $\tau_{refine}$. We also discuss particle level refinement and how the utilization of a SPH-like method is necessary to accurately calculate the flux when interacting with individual particles. Using our experience with GASOLINE we were able to develop simpler and more efficient algorithms for the ChaNGa implementation. These tests confirm the useful scaling of the TREVR method now implemented in Charm++.

We tested the code so developed by comparing the simulation results with analytical models for simple systems as well as the isothermal spheres test developed and described in detail in \citet{grond}. In particular, we note that the tests in  Grond et. al. are far better than those typically presented in other recent radiation code papers. In particular, error estimates are typically unavailable for many methods. We tested multiple values of $\theta_{open}$ and $\tau_{refine}$, studying the error for each value. Due to the quantitative tests developed by Grond et. al. we were able to suggest optimal values. We also recorded the number of rays or ray segments traced during the tree walks, using this as an analogue for the computational cost of the system and allowing us to see the cost increase for each refinement value and thus compare the cost to the error for each refinement value. It would be good to firmly establish the value of ray segments as a good proxy for actual work (e.g. wall clock time). We note that the ChaNGa implementation still has rough edges and some parts need to be optimized before such testing would be relevant.

Finally, we discussed the Complex Sources problem, where the merging of sources can cause systematic errors in the calculated flux for optically thick problems while also creating an angular dependence on its accuracy. We showed that merging nearly always causes a negative error in the flux received by a sink regardless of the optical depth of the system, and that the errors peaked for intermediate values of $\tau$. Such scenarios are common in astrophysical simulations. We propose several methods for the mitigation of this issue such as the application of a modified form of an optically thick refinement criterion on the source finding walk and ray tracing to a limited number of sources and studying the angular variation in optical depth. Both of these methods can be run during the tree build, allowing for the refinement to be checked once for all particles. We anticipate that this will suffer from the issue of over-refinement, dramatically increasing the computational cost of the radiative transfer code and offer a method for limiting this in the form of flux tracking. 

\section {Future Development}

Below is a selection of future additions to the ChaNGa radiation code that we were unable to implement due to time constraints or what fell outside the focus of this thesis.

\subsection{Volumetric Rays}

Infinitesimally thin rays, or ``pencil beams" that are used in many ray tracing methods including our own are computationally simple, only requiring a length to be calculated to give each segment's optical depth contribution. As shown in the discussion of complex sources, small changes in the path of the ray being traced can result in substantial changes in the calculated optical depth. Pencil beams suffer a similar issue where sources have a portion of their solid angle in the sky obscured by absorbers but whose centre of luminosity remains unobscured would give incorrect values for their flux compared to a fully refined computation. Unlike with the complex sources problem, this issue is due to the change in absorbers intersected by the ray rather than the interaction of sources and absorbers during the tree build.

A method of resolving this issue is to use non-pencil or ``volumetric" beams similar to those implemented in \citet{treeRay}. Each node's bounding box would interact with a section of the volume of the ray rather than a simple line segment, with equivalent for the particle level refinement. The flux from the source would be distributed across the volume in a similar method to the mass distribution of an SPH particle, with the majority of the flux contained within the centre. Calculating each node's contribution to absorption would require the beam to be integrated with bounds given by the node's bounding box limits. This would require substantially more computation than the pencil beam method and due to the increased volume contained within the beam, many more nodes would be intersected. 

It should be noted that for volumetric rays the correct method to average absorption is less clear as compact, dense objects will quite often only partially block a volumetric ray, regardless of how optically thick the object itself is. This issue was not fully addressed by Haid et al.

\subsection{Combined Particle Ray Trace}

Currently, the particle level contribution for the optical depth is computed for all sink particles individually and run during the tree walk for each sink. This results in the need to refine to the bucket level for all nodes within the P-P radius of all sinks, substantially adding to the computational cost of the system.

An alternative method would be to create a list of absorbers that require particle level refinement for all sinks in the bucket at once, using a P-P refinement radius equal to the value of d where,
\begin{equation}
    d = r_{COL}+h_{SPH},
\end{equation}
with $r_{COL}$ and $h_{SPH}$ referring to the sink-centre of bucket distance and the smoothing length of each particle respectively. The list created would contain absorbers that may not apply to some particles but once the costly particle collection step has been performed, the list of absorbers can be filtered for each sink with minimal cost. This would mean that during the tree walk, nodes within the P-P radius would not require full refinement, only refinement until some of their sibling nodes do not intersect the radius. The non-PP contribution can then be calculated as normal and the walk continued.

\subsection{Multiple Radiation Bands}

The code is currently written to only allow for tracing of one radiation band at any time. This reduces its utility in systems that are optically thick to certain wavelengths but thin to others of interest. Much of the existing code could be easily extended to utilize vector-based radiation properties, including during the tree build. Each node would require a band-specific absorption coefficient along with band specific luminosities and centres of luminosity and each particle would likewise require band-specific values for their radiation properties.

The code for the tree walk and tree build could be left much the same, only requiring additional code to act on all bands instead of one. The optically thick refinement criterion would become more complex as multiple bands would give varying results. The safest choice would be to refine whenever any band requires it as the actual optical depth summation work is small.

\subsection{Source List Hash Table}

Currently, the way that particle and node data are stored for the optically thick tree walk results in data duplication, with sinks which each have the same source both storing all of the source's properties. A far more efficient method would be to replace this with a hash table system, where the source properties are stored in a hash table and only the table key is recorded in the source list. This would greatly reduce data usage in systems with a large number of sources or where the refinement criterion for source merging is particularly stringent while having little impact on the computational cost of the code.

Another benefit of this method is that with a full list of sources and sinks, the full system can be walked at once, creating something similar to the full sky column density maps seen in \citet{treeRay}. This would have a scaling of $\mathcal{O}(N_{sink}\log(N)$ when traced out from the sink, substantially reducing the computational cost of the system.